{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s42BMZjn6LqE"
   },
   "source": [
    "**Earthquake Damage Dataset Overview**\n",
    "\n",
    "\n",
    "\n",
    "This dataset is a large collection of past earthquakes from all over the world. Reviewing this information is the crucial first step to understanding the patterns before we clean the data and train the machine learning models.\n",
    "\n",
    "**Key Details:**\n",
    "\n",
    "**Total Records:** 260,601 individual earthquakes.\n",
    "\n",
    "**Features**: 40 different pieces of information collected for each event.\n",
    "\n",
    "What's Inside: The data tracks exactly where the earthquake happened (latitude and longitude), when it happened, how deep underground it started, and its overall strength (magnitude).\n",
    "\n",
    "The Main Goal (Target Variable): The primary thing we want to predict is the severity of the earthquake—specifically, classifying whether an event will cause \"significant\" or \"non-significant\" damage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUYZfhe86NUR"
   },
   "source": [
    "**STEP 1 — Import Libraries**\n",
    "\n",
    "\n",
    "\n",
    "The project utilizes a standard data science stack. Core data manipulation was handled using Pandas and NumPy, while Matplotlib and Seaborn were used for visual data exploration. For the predictive modeling phase, Scikit-Learn and XGBoost were implemented to build and test multiple classifiers. Additionally, SMOTE (Synthetic Minority Over-sampling Technique) was imported from the Imbalanced-Learn library to ensure the models were trained on a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "evz_GiNX6Pan"
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0G3xz9eo6r_Q"
   },
   "source": [
    "**STEP 2** — **Load** **Dataset**\n",
    "\n",
    "\n",
    "In this step, we import the raw earthquake data into our workspace. Since we are working in Google Colab, we first mount Google Drive to access our stored files. The dataset is provided in two separate CSV files: one containing the building characteristics (train_values.csv) and another containing the target variable (train_labels.csv). We load both files using Pandas and then use the merge function to combine them into a single, unified dataframe based on their shared building_id. Finally, we preview the data to confirm the merge was successful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "VKLMQOAtGizV",
    "outputId": "11bc336e-2938-4a77-a459-08746eaf6490"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "train_values = pd.read_csv(\"train_values.csv\")\n",
    "\n",
    "# Check datasets\n",
    "print(train_labels.shape)\n",
    "print(train_values.shape)\n",
    "\n",
    "# Merge datasets correctly\n",
    "df = pd.merge(train_values, train_labels, on=\"building_id\")\n",
    "\n",
    "# Verify merge\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6o8heLa-Cw9"
   },
   "source": [
    "**STEP 3 — Basic Data Inspection**\n",
    "\n",
    "The dataset was examined to understand its structure and contents.\n",
    "\n",
    "Basic checks were performed to review data types, missing values, duplicates, and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3xNincE9tif"
   },
   "outputs": [],
   "source": [
    "# it shows shape of the dataset .\n",
    "df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCOY8tFzjlBI"
   },
   "outputs": [],
   "source": [
    "# it shows information about dataset.\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnBfZ7xskITT"
   },
   "outputs": [],
   "source": [
    "# get statistical summary of numerical columns in a DataFrame.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjGB_XIFkMD7"
   },
   "outputs": [],
   "source": [
    "#this is to view the null values in the datasets\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nu47LfKwkwwf"
   },
   "outputs": [],
   "source": [
    "#display the last 5 rows of a DataFrame.\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psNwwXDOkZcH"
   },
   "outputs": [],
   "source": [
    "#this is to view the sample in the datasets\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_V-pIe0k3E1"
   },
   "outputs": [],
   "source": [
    "#this is to view the columns in the datasets\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K19D1RsIlVUQ"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the geo_level_1_id column.\n",
    "df['geo_level_1_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zUOv3cPll-k"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the geo_level_2_id column.\n",
    "df['geo_level_2_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49xnYc3qlARR"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the geo_level_3_id column.\n",
    "df['geo_level_3_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSNWHY-tl4AP"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the count_floors_pre_eq column.\n",
    "df['count_floors_pre_eq'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyHncqxwmNIx"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appear age column.\n",
    "df['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WaaycSZomV2O"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the area percentage column.\n",
    "df['area_percentage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMwNdmSUmnCE"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the height_percentage column.\n",
    "df['height_percentage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ts5t4DGmtQK"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the land_surface_condtion column.\n",
    "df['land_surface_condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSQBVwSKnCkY"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the founation_type column.\n",
    "df['foundation_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqvT5NVCnMKn"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the roof_type column.\n",
    "df['roof_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-CBg7PEnY5Z"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the ground_floor_type  column.\n",
    "df['ground_floor_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2NmmntMnfjH"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in theother _floor_type  column.\n",
    "df['other_floor_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1qna29tnlV2"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the position column.\n",
    "df['position'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGx9NkPmnrpm"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the plan_configuration column.\n",
    "df['plan_configuration'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZbXrE2lnzbu"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_superstructure_adobo_mud  column.\n",
    "df['has_superstructure_adobe_mud'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpnkBnU0oNye"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_supersttructure_mud_mortar column.\n",
    "df['has_superstructure_mud_mortar_stone'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPJEQ3qwoYEL"
   },
   "outputs": [],
   "source": [
    "\n",
    "#it is used to count how many times each unique value appears in the has_superstructure_stone_flag column.\n",
    "df['has_superstructure_stone_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZrThNUcosQb"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_superstructure_cement_mortar_stone column.\n",
    "df['has_superstructure_cement_mortar_stone'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApcSao0Uo8lD"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_superstrcture_mud_mortar_brick column.\n",
    "df['has_superstructure_mud_mortar_brick'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6m62jEjdpJbY"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_superstructure_timber column.\n",
    "df['has_superstructure_timber'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJCgpFdJpXNs"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_superstrcture_bamboo column.\n",
    "df['has_superstructure_bamboo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrqyS_uvpdzd"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_superstrcture_rc_non_engineered  column.\n",
    "df['has_superstructure_rc_non_engineered'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEW_SKM5plR9"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_superstructure_rc_engineered  column.\n",
    "df['has_superstructure_rc_engineered'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwocSAbYptnA"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_superstructure_other column.\n",
    "df['has_superstructure_other'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQAPRMmipzEZ"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the legal_ownership_status column.\n",
    "df['legal_ownership_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gVAvM1-p5AW"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the count_familes column.\n",
    "df['count_families'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BU6ctWQ1p-ZW"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_secondary_use column.\n",
    "df['has_secondary_use'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdME-qFIqETo"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_secondary_use_agriculture  column.\n",
    "df['has_secondary_use_agriculture'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTa-6_J0qKSn"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_secondary_use_hotel  column.\n",
    "df['has_secondary_use_hotel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyUJmlCoqWAH"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_Secondary_use_rental  column.\n",
    "df['has_secondary_use_rental'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2KIbDmYqjlm"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_Seondary_use_school  column.\n",
    "df['has_secondary_use_school'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RA_aagNJq4Fb"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_secondary_use_industry   column.\n",
    "df['has_secondary_use_industry'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccAfA8Fmq-2O"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_secondary_use_health_post   column.\n",
    "df['has_secondary_use_health_post'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruDFpM6yrVkE"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_secondary_use_gov_office column.\n",
    "df['has_secondary_use_gov_office'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yZYZDj1rcJ2"
   },
   "outputs": [],
   "source": [
    "\n",
    "#it is used to count how many times each unique value appears in the has_Secondary_use_use_police column.\n",
    "df['has_secondary_use_use_police'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u58DTUh6rjJn"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the has_secondary_use_other  column.\n",
    "df['has_secondary_use_other'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vWvVE_trn0Z"
   },
   "outputs": [],
   "source": [
    "#it is used to count how many times each unique value appears in the damage_grade  column.\n",
    "df['damage_grade'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3t2zNI-K-Wal"
   },
   "source": [
    "**STEP 4 — Data Cleaning**\n",
    "\n",
    "\n",
    "**Dropping Unique Identifiers:** We remove the building_id column because it is simply a random, unique identifier assigned to each row. It holds no mathematical or predictive value regarding earthquake damage, and leaving it in the dataset would only act as noise that could confuse the algorithm.\n",
    "\n",
    "\n",
    "**Checking for Duplicates**: We run a check for duplicate rows to ensure data integrity. Exact duplicate records can artificially inflate the importance of certain data points, leading to biased predictions and overfitting. Identifying and removing them early is a crucial best practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0Zwpya8ggkO"
   },
   "outputs": [],
   "source": [
    "df.drop(\"building_id\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwJrA42xg2GH"
   },
   "source": [
    "**Check duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36O32wWGg_bn"
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNwim_9thHob"
   },
   "source": [
    "**If duplicates exist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_iYLIT0ShM3K"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzrpAcOZ-ekL"
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gr31dC3E-ht4"
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4A — Distribution of Numerical Features\n",
    "\n",
    "num_cols = df.select_dtypes(exclude='object').columns\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "pos = 1\n",
    "for col in num_cols:\n",
    "\n",
    "    plt.subplot(8,4,pos)\n",
    "    sns.histplot(df[col], kde=True)\n",
    "\n",
    "    plt.title(f'Distribution of {col}')\n",
    "\n",
    "    pos += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4(B) — Feature vs Target Relationship\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "sns.boxplot(x='damage_grade', y='age', data=df)\n",
    "\n",
    "plt.title('Building Age vs Damage Grade')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_pcxl3UhR5c"
   },
   "source": [
    "**STEP 5 — Check Target Distribution**\n",
    "\n",
    "\n",
    "Next, we need to examine our target variable, damage_grade, to understand the distribution of the structural damage classes. The seaborn count plot gives us a quick visual representation, while value_counts(normalize=True) provides the exact proportions of each damage level. This step is critical for identifying class imbalance. If one type of damage is heavily overrepresented compared to the others, it confirms our need to apply SMOTE to balance the dataset before training our predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrgSy2jzhWon"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=\"damage_grade\", data=df)\n",
    "plt.show()\n",
    "\n",
    "df[\"damage_grade\"].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LDw5xJRhbJL"
   },
   "source": [
    "**Example imbalance:\n",
    "\n",
    "Grade 2 → 56%\n",
    "Grade 3 → 33%\n",
    "Grade 1 → 11%\n",
    "\n",
    "\n",
    "# **This is class imbalance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvBvDijBhuLn"
   },
   "source": [
    "\n",
    "**STEP 6 — Feature Engineering & Categorical Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5LcCHYHiCYj"
   },
   "source": [
    "**Separate categorical and numerical columns**\n",
    "\n",
    "\n",
    "Machine learning algorithms require numerical input, so we must convert our categorical text data (such as building materials or region types) into a format the models can process. In this step, we automatically separate our text and numeric columns. Then, we apply One-Hot Encoding using pandas get_dummies. By setting drop_first=True, we prevent multicollinearity (the dummy variable trap), which is especially important for ensuring our Logistic Regression model performs optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N662PVQhhikS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Encoding categorical variables...\")\n",
    "\n",
    "# 1. Automatically grab all text (object) columns and numeric columns\n",
    "cat_cols = df.select_dtypes(include=\"object\").columns\n",
    "num_cols = df.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "# 2. Apply One-Hot Encoding using pandas get_dummies\n",
    "# drop_first=True prevents the dummy variable trap (highly recommended for Logistic Regression)\n",
    "df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "print(\"Encoding complete. New dataset shape: {}\".format(df.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDbNXEUliQw8"
   },
   "source": [
    "**STEP 7 — Feature and Target Split**\n",
    "\n",
    "\n",
    "To set up the predictive modeling phase for the structural damage assessment, the dataset was explicitly split into dependent and independent variables. The damage_grade column was isolated as the target variable (y), representing the severity of the damage. All other encoded categorical and numerical features were grouped into the feature matrix (X) to serve as the structural and regional predictors for the machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUA8ja-SiUlk"
   },
   "outputs": [],
   "source": [
    "X = df.drop(\"damage_grade\", axis=1)\n",
    "y = df[\"damage_grade\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fO5jHXVriYKb"
   },
   "source": [
    "**STEP 8 — Train Test Split**\n",
    "\n",
    "\n",
    "\n",
    "With our features and target defined, we now split the data into training and testing sets. We are reserving 20% of the data to evaluate our model's performance later on unseen data. Crucially, we use stratify=y during this split. Because our damage_grade classes may be imbalanced, stratifying ensures that the proportion of each damage category remains exactly the same in both the 80% training set and the 20% testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnkL7_z6ifY1"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8ZGY9ycii_H"
   },
   "source": [
    "Stratify keeps same class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHwdYkLwip9r"
   },
   "source": [
    "**STEP 9 — Handle Class Imbalance using SMOTE**\n",
    "\n",
    "SMOTE creates synthetic samples of minority class.\n",
    "\n",
    "\n",
    "Because earthquake damage often leans heavily toward certain grades (for instance, moderate damage might be far more common than complete destruction), our model could become biased and simply learn to predict the majority class every time. To fix this, we apply SMOTE (Synthetic Minority Over-sampling Technique) strictly to our training data. SMOTE analyzes the feature space and creates synthetic, realistic data points for the minority classes until all categories are perfectly balanced. The print statements at the end confirm that our classes are now evenly distributed before we begin training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFrsV-NeiscG"
   },
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", y_train.value_counts())\n",
    "print(\"After SMOTE:\", y_train_smote.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0295862"
   },
   "source": [
    "#### Impute Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbFADbEkjJQ2"
   },
   "source": [
    "**STEP 10 — Feature Scaling**\n",
    "\n",
    "Important for Logistic Regression, Gradient Boosting\n",
    "\n",
    "\n",
    "\n",
    "Machine learning models, particularly linear algorithms like Logistic Regression, are highly sensitive to the scale of the input data. For example, a feature like \"building age\" might range from 1 to 100, while a structural measurement might be a fraction of a meter. To prevent features with larger numbers from dominating the model, we apply standard scaling.Using StandardScaler, we transform our numerical features so they have a mean of 0 and a standard deviation of 1 using the formula $z = \\frac{x - \\mu}{\\sigma}$. Crucially, we use fit_transform on our training data to learn the scaling parameters, but only transform on our testing data to prevent data leakage from the test set into our model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnGq8_kcinHw"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxzzQh9qj57-"
   },
   "source": [
    "**STEP 11 — Model Building**\n",
    "\n",
    "With our data cleaned, balanced, and scaled, we are ready to build our predictive models. To ensure we find the best algorithm for predicting earthquake damage grades, we will train and compare four different classifiers:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJLLf_M_j7xL"
   },
   "source": [
    "We train 4 models:\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "Random Forest\n",
    "\n",
    "Gradient Boost\n",
    "\n",
    "XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnXgB4OCkIAe"
   },
   "source": [
    "**Model 1 — Logistic Regression**\n",
    "\n",
    "\n",
    "\n",
    "**Logistic Regression**: Serves as our interpretable, linear baseline model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXVOYuChkLMg"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "lr.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\",\n",
    "      accuracy_score(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBN0Y7hFkYoC"
   },
   "source": [
    "****Model 2 — Random Forest****\n",
    "\n",
    "\n",
    "**Random Forest:** A robust \"bagging\" ensemble method that builds multiple decision trees in parallel to reduce overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVcmVNvWkbJ3"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "rf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Accuracy:\",\n",
    "      accuracy_score(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEjPn6ECkd8I"
   },
   "source": [
    "**Model 3 — Gradient Boosting**\\\n",
    "\n",
    "\n",
    "\n",
    "**Gradient Boosting:** A powerful \"boosting\" ensemble method that builds trees sequentially, with each new tree correcting the errors of the previous ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9TYedMJkRbo"
   },
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "gb.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boost Accuracy:\",\n",
    "      accuracy_score(y_test, y_pred_gb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrAZrqPnklnl"
   },
   "source": [
    "**Model 4 — XGBoost (BEST MODEL)**\n",
    "\n",
    "\n",
    "**XGBoost:** An optimized, highly efficient implementation of gradient boosting that often provides state-of-the-art performance on tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyLEMtgikphC"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "# Shift training labels from [1, 2, 3] to [0, 1, 2] to satisfy XGBoost\n",
    "y_train_smote_adj = y_train_smote - 1\n",
    "# Generate predictions (which will be 0, 1, 2) and shift them back to [1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vEQP1H5ktAx"
   },
   "source": [
    "**STEP 12 — Model Comparison**\n",
    "\n",
    "After training our four models, we need to evaluate how well they generalized to our unseen testing data. In this step, we calculate the overall accuracy score for each classifier (Logistic Regression, Random Forest, Gradient Boosting, and XGBoost). To make the results easy to interpret, we compile these metrics into a pandas DataFrame and sort them in descending order. This gives us a clear leaderboard, allowing us to immediately identify which algorithm is the most effective at predicting earthquake damage grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oYYcHHRkweH"
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "\n",
    "\"Model\":[\n",
    "\"Logistic Regression\",\n",
    "\"Random Forest\",\n",
    "\"Gradient Boost\",\n",
    "\"XGBoost\"\n",
    "],\n",
    "\n",
    "\"Accuracy\":[\n",
    "accuracy_score(y_test, y_pred_lr),\n",
    "accuracy_score(y_test, y_pred_rf),\n",
    "accuracy_score(y_test, y_pred_gb),\n",
    "accuracy_score(y_test, y_pred_xgb)\n",
    "]\n",
    "\n",
    "})\n",
    "\n",
    "results.sort_values(by=\"Accuracy\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sTlM2YRk2Uo"
   },
   "source": [
    "**STEP 13 — Detailed Evaluation**\n",
    "\n",
    "While overall accuracy gives us a quick leaderboard, it doesn't tell the whole story—especially when predicting earthquake damage, where severe damage might be less frequent than minor damage. To truly understand how our best-performing model (XGBoost) is operating, we generate a classification report.\n",
    "\n",
    "This report breaks down the model's performance for each individual damage grade using three key metrics:\n",
    "\n",
    "**Precision**: When the model predicts a specific damage grade, how often is it correct?\n",
    "\n",
    "**Recall:** Out of all the buildings that actually had a specific damage grade, how many did the model successfully find?\n",
    "\n",
    "**F1-Score:** The harmonic mean of precision and recall, giving us a single metric to judge the model's balance between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ufiiOJck6Yk"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5T1UHN_k9YO"
   },
   "source": [
    "**STEP 14 — Confusion Matrix**\n",
    "\n",
    "While the classification report provides the exact metrics, a Confusion Matrix gives us the best visual representation of our XGBoost model's performance. By plotting the actual damage grades against the model's predicted grades using a Seaborn heatmap, we can see exactly where the algorithm is getting \"confused.\"\n",
    "\n",
    "The diagonal line of dark blue squares represents our correct predictions. Any numbers outside of that diagonal show us the misclassifications. For earthquake damage assessment, this is critical—it allows us to see if the model is accidentally predicting minor damage when a building was actually completely destroyed (a dangerous false negative), or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bGsG3EQlAqY"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ge3dUrKBlEBk"
   },
   "source": [
    "**STEP 15 — Feature Importance**\n",
    "\n",
    "\n",
    "While having a highly accurate model is fantastic, it is equally important to understand what is actually driving those predictions. In this final analysis step, we extract the built-in feature importances from our trained XGBoost model. By plotting the top 10 most influential features in a horizontal bar chart, we can clearly see which attributes—such as the building's age, foundation type, or geographic location—played the biggest role in determining the severity of the earthquake damage. This interpretability turns our \"black box\" model into actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvV1ud4PlG_I"
   },
   "outputs": [],
   "source": [
    "importance = xgb.feature_importances_\n",
    "\n",
    "feat_importance = pd.Series(\n",
    "importance,\n",
    "index=X.columns\n",
    ")\n",
    "\n",
    "feat_importance.nlargest(10).plot(kind=\"barh\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwQZ4PN2xCXt"
   },
   "source": [
    "**STEP 16 — Hyperparameter Tuning**\n",
    "\n",
    "\n",
    "To maximize the predictive capabilities of the leading XGBoost model, a hyperparameter tuning phase was executed using RandomizedSearchCV. A parameter grid was defined to optimize key algorithm constraints, including learning_rate, max_depth, n_estimators, and subsample ratios. The search was conducted over 3-fold cross-validation to rigorously validate performance and prevent overfitting on the synthetic SMOTE data. This optimization process successfully identified the most effective model configuration, yielding a fine-tuned XGBoost classifier for the final evaluation on the testing holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DysRd30twCB-"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"Starting STEP 16: Hyperparameter Tuning for XGBoost...\")\n",
    "\n",
    "# 1. Define the parameter grid (the \"dials\" we want to test)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],        # Number of trees\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2], # Step size at each iteration\n",
    "    'max_depth': [4, 6, 8, 10],              # Maximum depth of a tree\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0]        # Fraction of samples used per tree (prevents overfitting)\n",
    "}\n",
    "\n",
    "# 2. Instantiate the base model\n",
    "xgb_base = XGBClassifier(random_state=42)\n",
    "\n",
    "# 3. Set up RandomizedSearchCV\n",
    "# n_iter=5 means it will randomly try 5 different combinations from the grid above.\n",
    "# You can increase this to 10 or 20 if your computer is fast!\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=5,\n",
    "    scoring='accuracy',\n",
    "    cv=3,                 # 3-fold cross-validation\n",
    "    verbose=2,            # Prints progress while training\n",
    "    random_state=42,\n",
    "    n_jobs=-1             # Uses all available CPU cores to speed up training\n",
    ")\n",
    "\n",
    "# 4. Fit the search to your SMOTE-balanced training data\n",
    "random_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# 5. Extract and print the best results\n",
    "print(\"\\nTuning Complete!\")\n",
    "print(\"Best Parameters Found: {}\".format(random_search.best_params_))\n",
    "print(\"Best Cross-Validation Accuracy: {:.4f}\".format(random_search.best_score_))\n",
    "\n",
    "# 6. Save the best model to use for your final predictions\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# Check final performance on the test set\n",
    "final_predictions = best_xgb_model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Final Test Accuracy with Tuned Model: {:.4f}\".format(accuracy_score(y_test, final_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcWFUKwuDDU5"
   },
   "source": [
    "**STEP 17 — Saving the Model and Scaler (Serialization)**\n",
    "\n",
    "Now that we have successfully tuned our XGBoost model to its optimal performance, we need to save it. Training a model can take a significant amount of time and compute power, so we use a process called serialization to save the trained model to our local disk.\n",
    "\n",
    "Crucially, we must also save the StandardScaler that we fitted in Step 10. When new building data is fed into the model in the future, it must be scaled using the exact same mean and variance as our training data; otherwise, the model's predictions will be completely inaccurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJxAidPVtkNc"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "print(\"Starting STEP 17: Saving the model and scaler...\")\n",
    "\n",
    "# 1. Save the tuned XGBoost model\n",
    "joblib.dump(best_xgb_model, 'tuned_xgb_model.pkl')\n",
    "\n",
    "# 2. Save the scaler used for numerical features\n",
    "joblib.dump(scaler, 'standard_scaler.pkl')\n",
    "\n",
    "print(\"Success! Model and Scaler saved to disk.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOeuLlaMt2mB"
   },
   "source": [
    "**STEP 18 — Simulating a Real-World Prediction**\n",
    "\n",
    "To prove that our saved model works exactly as expected, we will simulate a real-world scenario. Imagine a civil engineer has just surveyed a building and submitted its data to our system. We will load our saved model and scaler from the disk, preprocess this \"new\" data, and generate a structural damage prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OazXzBcXt2-9"
   },
   "outputs": [],
   "source": [
    "print(\"Starting STEP 18: Simulating a new prediction...\")\n",
    "\n",
    "# 1. Load the saved components from disk\n",
    "loaded_model = joblib.load('tuned_xgb_model.pkl')\n",
    "loaded_scaler = joblib.load('standard_scaler.pkl')\n",
    "\n",
    "# 2. Simulate \"new\" incoming data\n",
    "# (Grabbing the very first row of our unscaled test set for this example)\n",
    "# Reshape is required because sklearn models expect a 2D array (rows and columns)\n",
    "new_building_data = X_test_unscaled.iloc[0].values.reshape(1, -1)\n",
    "\n",
    "# 3. Scale the new data using the loaded scaler\n",
    "new_building_scaled = loaded_scaler.transform(new_building_data)\n",
    "\n",
    "# 4. Make the final prediction\n",
    "predicted_damage = loaded_model.predict(new_building_scaled)\n",
    "\n",
    "print(\"Predicted Damage Grade for the new building: {}\".format(predicted_damage[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwfIhr72uGWs"
   },
   "source": [
    "**STEP 19 — Project Conclusion and Deployment Strategy**\n",
    "\n",
    "**Conclusion & Next Steps**\n",
    "This project successfully demonstrates an end-to-end machine learning pipeline for predicting earthquake damage. By addressing severe class imbalance with SMOTE and rigorously tuning an XGBoost classifier, we created a robust model capable of identifying structural vulnerabilities based on geographic and architectural features.\n",
    "\n",
    "**Future Work**: Web Deployment\n",
    "To make this model accessible to non-technical stakeholders, the immediate next step is to wrap the saved .pkl files in a web application framework. Using a tool like Streamlit or Flask, we can build an interactive dashboard where users can manually input building characteristics (like age, foundation type, and roof structure) through dropdown menus and sliders to receive an instant damage assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zr8j1EvHugL9"
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# EXAMPLE: Future Streamlit App Structure (app.py)\n",
    "# ==========================================\n",
    "\n",
    "# import streamlit as st\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "\n",
    "# st.title(\"Earthquake Damage Predictor\")\n",
    "\n",
    "# # Load model\n",
    "# model = joblib.load('tuned_xgb_model.pkl')\n",
    "# scaler = joblib.load('standard_scaler.pkl')\n",
    "\n",
    "# # Get user input from the web interface\n",
    "# age = st.number_input(\"Building Age\")\n",
    "# floors = st.number_input(\"Number of Floors\")\n",
    "# # ... (collect all other features)\n",
    "\n",
    "# if st.button(\"Predict Damage\"):\n",
    "#     features = np.array([[age, floors, ...]])\n",
    "#     scaled_features = scaler.transform(features)\n",
    "#     prediction = model.predict(scaled_features)\n",
    "#     st.success(\"The predicted damage grade is: {}\".format(prediction[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMhH9v4s3pJb"
   },
   "source": [
    "**Conclusion: Earthquake Damage Prediction**\n",
    "\n",
    "This project successfully developed a machine learning solution to predict the severity of earthquake damage using a large-scale dataset of over 260,000 historical seismic records.\n",
    "\n",
    "**Project Highlights:**\n",
    "\n",
    "Data Processing: By analyzing 40 distinct features—such as magnitude, depth, and exact geographic coordinates—the raw data was cleaned and structured to reveal underlying patterns in seismic activity.\n",
    "\n",
    "**Handling Imbalance:**\n",
    "Because different levels of earthquake intensity are not equally common, data balancing techniques like SMOTE were applied to ensure the model learned to recognize all damage categories fairly, without ignoring the minority classes.\n",
    "\n",
    "**Model Training & Evaluation:**\n",
    " Multiple machine learning algorithms, including XGBoost, were trained on the processed data. The models were strictly evaluated using Accuracy and Weighted F1-Scores to guarantee reliable predictions across the board.\n",
    "\n",
    "**Final Selection:**\n",
    "An automated evaluation pipeline successfully identified and extracted the single best-performing model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKVIYSc44Fw4"
   },
   "source": [
    "**Real-World Impact:**\n",
    "Ultimately, this trained model can take in fresh seismic data and accurately classify the expected intensity of an event. Predictive tools like this are vital for early warning systems, helping emergency responders allocate resources quickly, plan effective disaster response strategies, and potentially minimize structural damage and save lives."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
